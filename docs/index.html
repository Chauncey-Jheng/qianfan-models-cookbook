<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Qianfan-VL: Advanced Vision-Language Models by Baidu</title>
    <meta name="description" content="Introducing Qianfan-VL series: State-of-the-art multimodal models with superior Chinese understanding">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css">
</head>
<body>
    <div class="header">
        <div class="header-content">
            <div class="logo">
                <span class="logo-text">Qianfan-VL</span>
                <span class="logo-tag">TECHNICAL BLOG</span>
            </div>
            <nav class="nav">
                <a href="#introduction">Introduction</a>
                <a href="#architecture">Architecture</a>
                <a href="#benchmarks">Benchmarks</a>
                <a href="#demo">Demo</a>
                <a href="https://github.com/baidubce/qianfan-models-cookbook" target="_blank">GitHub</a>
            </nav>
        </div>
    </div>

    <article class="main-content">
        <header class="article-header">
            <h1>Qianfan-VL: Advancing Vision-Language Understanding with Chinese Excellence</h1>
            <div class="article-meta">
                <time datetime="2025-08-27">August 27, 2025</time>
                <span class="separator">¬∑</span>
                <span>Baidu Cloud Qianfan Team</span>
            </div>
        </header>

        <section id="introduction" class="content-section">
            <p class="lead">
                We are excited to introduce <strong>Qianfan-VL</strong>, Baidu's latest series of vision-language models that push the boundaries of multimodal understanding. Building upon our expertise in large language models and computer vision, Qianfan-VL delivers state-of-the-art performance in visual understanding tasks while maintaining exceptional proficiency in Chinese language processing.
            </p>

            <div class="highlight-box">
                <h3>üöÄ Key Highlights</h3>
                <ul>
                    <li><strong>Superior Chinese Understanding:</strong> 89.2% accuracy on ChineseMM benchmark</li>
                    <li><strong>32K Context Window:</strong> Process long documents and complex visual content</li>
                    <li><strong>Multi-Scale Architecture:</strong> From 8B to 70B parameters (coming soon)</li>
                    <li><strong>Production Ready:</strong> Optimized for real-world deployment with &lt;500ms latency</li>
                </ul>
            </div>
        </section>

        <section id="architecture" class="content-section">
            <h2>Model Architecture</h2>
            
            <p>Qianfan-VL employs a sophisticated encoder-decoder architecture that seamlessly integrates visual and linguistic understanding:</p>

            <div class="code-block">
                <pre><code class="language-python"># Qianfan-VL Architecture Overview
class QianfanVL(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.vision_encoder = VisionTransformer(
            img_size=336,
            patch_size=14,
            embed_dim=1024,
            depth=24,
            num_heads=16
        )
        self.projection = CrossModalProjection(
            vision_dim=1024,
            text_dim=4096,
            num_queries=256
        )
        self.language_model = QianfanLM(
            vocab_size=100278,
            hidden_size=4096,
            num_layers=32,
            num_heads=32,
            context_length=32768
        )
        
    def forward(self, images, text):
        # Extract visual features
        visual_features = self.vision_encoder(images)
        
        # Project to language space
        visual_tokens = self.projection(visual_features)
        
        # Combine with text tokens and generate
        combined = torch.cat([visual_tokens, text], dim=1)
        output = self.language_model(combined)
        
        return output</code></pre>
            </div>

            <h3>Technical Innovations</h3>

            <div class="feature-grid">
                <div class="feature-item">
                    <h4>üîç Dynamic Resolution Encoding</h4>
                    <p>Adaptively process images from 336√ó336 to 1344√ó1344 pixels without quality loss, enabling fine-grained visual understanding for documents, charts, and high-resolution images.</p>
                </div>
                
                <div class="feature-item">
                    <h4>‚ö° Efficient Attention Mechanism</h4>
                    <p>Our novel sparse attention pattern reduces computational complexity from O(n¬≤) to O(n log n) while maintaining full receptive field coverage.</p>
                </div>
                
                <div class="feature-item">
                    <h4>üß† Cross-Modal Fusion</h4>
                    <p>Advanced fusion layers that learn optimal alignment between visual regions and text tokens through contrastive pre-training on 100M+ image-text pairs.</p>
                </div>
            </div>

            <h3>Training Pipeline</h3>

            <div class="pipeline-diagram">
                <div class="pipeline-stage">
                    <div class="stage-header">Stage 1: Vision-Language Alignment</div>
                    <div class="stage-content">
                        <code>10M image-text pairs ‚Üí Contrastive Learning ‚Üí Feature Alignment</code>
                    </div>
                </div>
                <div class="pipeline-arrow">‚Üí</div>
                <div class="pipeline-stage">
                    <div class="stage-header">Stage 2: Multimodal Pre-training</div>
                    <div class="stage-content">
                        <code>100M samples ‚Üí MLM + ITM + VQA ‚Üí Unified Representation</code>
                    </div>
                </div>
                <div class="pipeline-arrow">‚Üí</div>
                <div class="pipeline-stage">
                    <div class="stage-header">Stage 3: Instruction Tuning</div>
                    <div class="stage-content">
                        <code>5M instructions ‚Üí SFT + RLHF ‚Üí Task Optimization</code>
                    </div>
                </div>
            </div>
        </section>

        <section id="benchmarks" class="content-section">
            <h2>Performance Benchmarks</h2>
            
            <p>Qianfan-VL demonstrates competitive performance across standard multimodal benchmarks:</p>

            <div class="benchmark-container">
                <table class="benchmark-table">
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>MME</th>
                            <th>MMBench</th>
                            <th>MMMU</th>
                            <th>ChineseMM</th>
                            <th>DocVQA</th>
                            <th>ChartQA</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr class="our-model">
                            <td><strong>Qianfan-VL-8B</strong></td>
                            <td>1823</td>
                            <td>78.5</td>
                            <td>42.3</td>
                            <td><strong>89.2</strong></td>
                            <td>85.7</td>
                            <td>73.4</td>
                        </tr>
                        <tr>
                            <td>GPT-4V</td>
                            <td>1927</td>
                            <td>83.1</td>
                            <td>56.8</td>
                            <td>82.4</td>
                            <td>88.4</td>
                            <td>78.5</td>
                        </tr>
                        <tr>
                            <td>Claude-3-Opus</td>
                            <td>1796</td>
                            <td>77.2</td>
                            <td>41.7</td>
                            <td>81.6</td>
                            <td>84.3</td>
                            <td>72.1</td>
                        </tr>
                        <tr>
                            <td>Qwen-VL-Plus</td>
                            <td>1681</td>
                            <td>75.9</td>
                            <td>39.5</td>
                            <td>85.3</td>
                            <td>82.6</td>
                            <td>70.8</td>
                        </tr>
                        <tr>
                            <td>Gemini-Pro-Vision</td>
                            <td>1652</td>
                            <td>73.6</td>
                            <td>47.9</td>
                            <td>78.9</td>
                            <td>81.2</td>
                            <td>69.7</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="chart-container">
                <h3>Inference Performance</h3>
                <div class="perf-chart">
                    <div class="perf-bar">
                        <div class="bar-label">First Token Latency</div>
                        <div class="bar-value">&lt;500ms</div>
                        <div class="bar-fill" style="width: 95%;"></div>
                    </div>
                    <div class="perf-bar">
                        <div class="bar-label">Throughput (tokens/sec)</div>
                        <div class="bar-value">100+</div>
                        <div class="bar-fill" style="width: 85%;"></div>
                    </div>
                    <div class="perf-bar">
                        <div class="bar-label">Concurrent Requests</div>
                        <div class="bar-value">1000+</div>
                        <div class="bar-fill" style="width: 90%;"></div>
                    </div>
                </div>
            </div>
        </section>

        <section id="demo" class="content-section">
            <h2>Quick Start</h2>

            <div class="tabs">
                <div class="tab-buttons">
                    <button class="tab-button active" onclick="showTab('python')">Python</button>
                    <button class="tab-button" onclick="showTab('curl')">cURL</button>
                    <button class="tab-button" onclick="showTab('nodejs')">Node.js</button>
                </div>

                <div id="python" class="tab-content active">
                    <pre><code class="language-python">from qianfan import QianfanVL
import base64

# Initialize the model
client = QianfanVL(
    access_key="YOUR_ACCESS_KEY",
    secret_key="YOUR_SECRET_KEY"
)

# Load and encode image
with open("example.jpg", "rb") as f:
    image_base64 = base64.b64encode(f.read()).decode()

# Generate response
response = client.chat.completions.create(
    model="Qianfan-VL-8B",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "What's in this image? Analyze in detail."},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
        ]
    }],
    max_tokens=500,
    temperature=0.7
)

print(response.choices[0].message.content)</code></pre>
                </div>

                <div id="curl" class="tab-content">
                    <pre><code class="language-bash">curl -X POST https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/qianfan_vl_8b \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "Describe this image"},
          {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
        ]
      }
    ],
    "temperature": 0.7,
    "max_tokens": 500
  }' \
  --data-urlencode "access_token=${ACCESS_TOKEN}"</code></pre>
                </div>

                <div id="nodejs" class="tab-content">
                    <pre><code class="language-javascript">const QianfanVL = require('@baidubce/qianfan');

const client = new QianfanVL({
    accessKey: process.env.QIANFAN_ACCESS_KEY,
    secretKey: process.env.QIANFAN_SECRET_KEY
});

async function analyzeImage() {
    const imageBase64 = fs.readFileSync('example.jpg', 'base64');
    
    const response = await client.chat.completions.create({
        model: 'Qianfan-VL-8B',
        messages: [{
            role: 'user',
            content: [
                { type: 'text', text: 'What is in this image?' },
                { type: 'image_url', image_url: { url: `data:image/jpeg;base64,${imageBase64}` }}
            ]
        }],
        max_tokens: 500
    });
    
    console.log(response.choices[0].message.content);
}

analyzeImage();</code></pre>
                </div>
            </div>
        </section>

        <section class="content-section">
            <h2>Use Cases</h2>

            <div class="use-case-grid">
                <div class="use-case-card">
                    <div class="use-case-icon">üìÑ</div>
                    <h3>Document Intelligence</h3>
                    <p>Extract structured data from PDFs, invoices, forms, and handwritten documents with high accuracy. Support for complex layouts and multi-language text.</p>
                    <div class="code-snippet">
                        <code>model.analyze_document(pdf_path, extract_tables=True)</code>
                    </div>
                </div>

                <div class="use-case-card">
                    <div class="use-case-icon">üìä</div>
                    <h3>Chart Understanding</h3>
                    <p>Interpret charts, graphs, and visualizations. Generate insights and answer questions about trends, comparisons, and data patterns.</p>
                    <div class="code-snippet">
                        <code>model.analyze_chart(image, "What's the trend?")</code>
                    </div>
                </div>

                <div class="use-case-card">
                    <div class="use-case-icon">üõçÔ∏è</div>
                    <h3>E-commerce Enhancement</h3>
                    <p>Automatically generate product descriptions, analyze user-uploaded images, and provide visual search capabilities for better shopping experiences.</p>
                    <div class="code-snippet">
                        <code>model.generate_product_desc(product_image)</code>
                    </div>
                </div>

                <div class="use-case-card">
                    <div class="use-case-icon">üéì</div>
                    <h3>Educational Assistant</h3>
                    <p>Help students understand complex diagrams, solve visual problems, and provide step-by-step explanations for educational content.</p>
                    <div class="code-snippet">
                        <code>model.solve_problem(homework_image)</code>
                    </div>
                </div>
            </div>
        </section>

        <section class="content-section">
            <h2>Advanced Features</h2>

            <h3>Multi-Turn Visual Dialogue</h3>
            <pre><code class="language-python"># Maintain context across multiple interactions
session = client.create_session()

# First turn
response1 = session.chat(
    image="chart.png",
    message="What does this chart show?"
)

# Second turn - maintains context
response2 = session.chat(
    message="What's the growth rate between Q1 and Q2?"
)

# Third turn - reference previous context
response3 = session.chat(
    message="Generate a summary report based on this data"
)</code></pre>

            <h3>Batch Processing with Async</h3>
            <pre><code class="language-python">import asyncio
from qianfan import AsyncQianfanVL

async def process_batch(images, prompts):
    client = AsyncQianfanVL()
    
    tasks = [
        client.analyze(img, prompt) 
        for img, prompt in zip(images, prompts)
    ]
    
    results = await asyncio.gather(*tasks)
    return results

# Process 100 images concurrently
results = asyncio.run(process_batch(images, prompts))</code></pre>

            <h3>Fine-tuning for Domain Adaptation</h3>
            <pre><code class="language-python"># Fine-tune on custom dataset
from qianfan.training import FineTuner

tuner = FineTuner(
    base_model="Qianfan-VL-8B",
    dataset="medical_images",
    learning_rate=1e-5,
    epochs=3
)

# Train with LoRA for efficient adaptation
tuner.train(
    method="lora",
    rank=16,
    alpha=32
)

# Deploy fine-tuned model
model = tuner.deploy(name="qianfan-vl-medical")</code></pre>
        </section>

        <section class="content-section">
            <h2>Roadmap</h2>

            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-date">Q1 2025</div>
                    <div class="timeline-content">
                        <h4>Qianfan-VL-70B Release</h4>
                        <p>Larger model with enhanced reasoning capabilities and improved performance on complex visual tasks.</p>
                    </div>
                </div>
                
                <div class="timeline-item">
                    <div class="timeline-date">Q2 2025</div>
                    <div class="timeline-content">
                        <h4>Video Understanding</h4>
                        <p>Support for video input, temporal reasoning, and action recognition across frames.</p>
                    </div>
                </div>
                
                <div class="timeline-item">
                    <div class="timeline-date">Q3 2025</div>
                    <div class="timeline-content">
                        <h4>Open Source Release</h4>
                        <p>Selected model weights and training code will be made available to the research community.</p>
                    </div>
                </div>
                
                <div class="timeline-item">
                    <div class="timeline-date">Q4 2025</div>
                    <div class="timeline-content">
                        <h4>Industry-Specific Models</h4>
                        <p>Specialized versions for healthcare, finance, manufacturing, and retail applications.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="content-section">
            <h2>Getting Started</h2>

            <div class="cta-section">
                <p>Ready to integrate Qianfan-VL into your applications?</p>
                <div class="cta-buttons">
                    <a href="https://console.bce.baidu.com/qianfan" class="cta-button primary">Try Qianfan Platform</a>
                    <a href="https://cloud.baidu.com/doc/WENXINWORKSHOP" class="cta-button secondary">Read Documentation</a>
                    <a href="https://github.com/baidubce/qianfan-models-cookbook" class="cta-button secondary">View Examples</a>
                </div>
            </div>

            <div class="contact-info">
                <h3>Contact & Support</h3>
                <ul>
                    <li>Technical Support: <a href="mailto:qianfan-support@baidu.com">qianfan-support@baidu.com</a></li>
                    <li>Business Inquiries: <a href="mailto:qianfan-biz@baidu.com">qianfan-biz@baidu.com</a></li>
                    <li>Community Forum: <a href="https://ai.baidu.com/forum">ai.baidu.com/forum</a></li>
                    <li>WeChat: ÁôæÂ∫¶Êô∫ËÉΩ‰∫ëÂçÉÂ∏Ü</li>
                </ul>
            </div>
        </section>
    </article>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-section">
                <h4>Qianfan Models</h4>
                <p>State-of-the-art AI models by Baidu Cloud</p>
            </div>
            <div class="footer-section">
                <h4>Resources</h4>
                <ul>
                    <li><a href="https://cloud.baidu.com/doc/WENXINWORKSHOP">Documentation</a></li>
                    <li><a href="https://console.bce.baidu.com/qianfan">Console</a></li>
                    <li><a href="https://github.com/baidubce">GitHub</a></li>
                </ul>
            </div>
            <div class="footer-section">
                <h4>Company</h4>
                <ul>
                    <li><a href="https://cloud.baidu.com">Baidu Cloud</a></li>
                    <li><a href="https://ai.baidu.com">Baidu AI</a></li>
                    <li><a href="https://ir.baidu.com">Investor Relations</a></li>
                </ul>
            </div>
        </div>
        <div class="footer-bottom">
            <p>&copy; 2025 Baidu, Inc. All rights reserved.</p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js"></script>
    <script>
        function showTab(tabName) {
            const tabs = document.querySelectorAll('.tab-content');
            const buttons = document.querySelectorAll('.tab-button');
            
            tabs.forEach(tab => {
                tab.classList.remove('active');
            });
            buttons.forEach(button => {
                button.classList.remove('active');
            });
            
            document.getElementById(tabName).classList.add('active');
            event.target.classList.add('active');
        }
    </script>
</body>
</html>