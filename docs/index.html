<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Qianfan-VL: Vision-Language Models | Baidu Cloud</title>
    <meta name="description" content="Qianfan-VL series: State-of-the-art multimodal models with superior Chinese understanding">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
</head>
<body>
    <header class="site-header">
        <div class="container">
            <div class="header-content">
                <h1 class="site-title">Qianfan-VL Technical Documentation</h1>
                <nav class="main-nav">
                    <a href="#overview">Overview</a>
                    <a href="#architecture">Architecture</a>
                    <a href="#benchmarks">Benchmarks</a>
                    <a href="#quickstart">Quick Start</a>
                    <a href="#api">API</a>
                    <a href="https://github.com/baidubce/qianfan-models-cookbook">GitHub</a>
                </nav>
            </div>
        </div>
    </header>

    <main class="main-content">
        <div class="container">
            <div class="hero-section">
                <h2 class="hero-title">Qianfan-VL: Advanced Vision-Language Understanding</h2>
                <p class="hero-subtitle">Baidu Cloud's State-of-the-Art Multimodal AI Models</p>
                <p class="hero-meta">Version 1.0 | Released August 2025</p>
            </div>

            <section id="overview" class="section">
                <h2>Overview</h2>
                <p>
                    Qianfan-VL represents Baidu's latest advancement in vision-language modeling, delivering exceptional performance 
                    in multimodal understanding tasks. Built on years of research in large language models and computer vision, 
                    Qianfan-VL achieves state-of-the-art results particularly in Chinese language and vision tasks.
                </p>
                
                <h3>Key Features</h3>
                <ul class="feature-list">
                    <li><strong>Model Sizes:</strong> 8B parameters (70B coming Q1 2025)</li>
                    <li><strong>Context Window:</strong> 32,768 tokens</li>
                    <li><strong>Image Resolution:</strong> Dynamic 336×336 to 1344×1344 pixels</li>
                    <li><strong>Languages:</strong> Chinese, English (optimized for Chinese)</li>
                    <li><strong>Deployment:</strong> Cloud API, Private deployment available</li>
                </ul>

                <h3>Performance Highlights</h3>
                <div class="metrics-grid">
                    <div class="metric-card">
                        <div class="metric-value">89.2%</div>
                        <div class="metric-label">ChineseMM Accuracy</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">&lt;500ms</div>
                        <div class="metric-label">First Token Latency</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">100+</div>
                        <div class="metric-label">Tokens/Second</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">32K</div>
                        <div class="metric-label">Context Window</div>
                    </div>
                </div>
            </section>

            <section id="architecture" class="section">
                <h2>Model Architecture</h2>
                
                <p>
                    Qianfan-VL employs a sophisticated encoder-decoder architecture with advanced cross-modal attention mechanisms. 
                    The model consists of three main components:
                </p>

                <h3>Architecture Components</h3>
                
                <div class="architecture-overview">
                    <pre class="architecture-diagram">
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│ Vision Encoder  │────▶│  Cross-Modal    │────▶│ Language Model  │
│  (ViT-based)    │     │   Projection    │     │   (32 layers)   │
└─────────────────┘     └─────────────────┘     └─────────────────┘
     336-1344px              256 queries            32K context
                    </pre>
                </div>

                <h4>Implementation Details</h4>
                <pre><code class="language-python">class QianfanVL(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Vision Transformer for image encoding
        self.vision_encoder = VisionTransformer(
            img_size=336,
            patch_size=14,
            embed_dim=1024,
            depth=24,
            num_heads=16
        )
        
        # Cross-modal projection layer
        self.projection = CrossModalProjection(
            vision_dim=1024,
            text_dim=4096,
            num_queries=256
        )
        
        # Language model backbone
        self.language_model = QianfanLM(
            vocab_size=100278,
            hidden_size=4096,
            num_layers=32,
            num_heads=32,
            context_length=32768
        )</code></pre>

                <h3>Training Process</h3>
                <ol class="training-stages">
                    <li>
                        <strong>Stage 1: Vision-Language Alignment</strong>
                        <p>Pre-training on 10M image-text pairs using contrastive learning</p>
                    </li>
                    <li>
                        <strong>Stage 2: Multimodal Pre-training</strong>
                        <p>Training on 100M+ samples with MLM, ITM, and VQA objectives</p>
                    </li>
                    <li>
                        <strong>Stage 3: Instruction Tuning</strong>
                        <p>Fine-tuning on 5M instructions using SFT and RLHF</p>
                    </li>
                </ol>
            </section>

            <section id="benchmarks" class="section">
                <h2>Performance Benchmarks</h2>
                
                <p>
                    Qianfan-VL has been extensively evaluated on standard multimodal benchmarks. 
                    The model demonstrates competitive performance across all metrics, with particularly strong results on Chinese tasks.
                </p>

                <h3>Benchmark Results</h3>
                <div class="table-wrapper">
                    <table class="benchmark-table">
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th>MME</th>
                                <th>MMBench</th>
                                <th>MMMU</th>
                                <th>ChineseMM</th>
                                <th>DocVQA</th>
                                <th>ChartQA</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="highlight-row">
                                <td><strong>Qianfan-VL-8B</strong></td>
                                <td>1823</td>
                                <td>78.5%</td>
                                <td>42.3%</td>
                                <td><strong>89.2%</strong></td>
                                <td>85.7%</td>
                                <td>73.4%</td>
                            </tr>
                            <tr>
                                <td>GPT-4V</td>
                                <td>1927</td>
                                <td>83.1%</td>
                                <td>56.8%</td>
                                <td>82.4%</td>
                                <td>88.4%</td>
                                <td>78.5%</td>
                            </tr>
                            <tr>
                                <td>Claude-3-Opus</td>
                                <td>1796</td>
                                <td>77.2%</td>
                                <td>41.7%</td>
                                <td>81.6%</td>
                                <td>84.3%</td>
                                <td>72.1%</td>
                            </tr>
                            <tr>
                                <td>Qwen-VL-Plus</td>
                                <td>1681</td>
                                <td>75.9%</td>
                                <td>39.5%</td>
                                <td>85.3%</td>
                                <td>82.6%</td>
                                <td>70.8%</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
                <h3>Inference Performance</h3>
                <ul class="performance-list">
                    <li><strong>Latency:</strong> &lt;500ms first token, ~10ms/token thereafter</li>
                    <li><strong>Throughput:</strong> 100+ tokens/second (single GPU)</li>
                    <li><strong>Concurrency:</strong> 1000+ QPS (cloud deployment)</li>
                    <li><strong>Memory:</strong> 16GB VRAM (INT8 quantized)</li>
                </ul>
            </section>

            <section id="quickstart" class="section">
                <h2>Quick Start Guide</h2>
                
                <h3>Installation</h3>
                <pre><code class="language-bash"># Install Qianfan SDK
pip install qianfan

# Set up authentication
export QIANFAN_ACCESS_KEY="your_access_key"
export QIANFAN_SECRET_KEY="your_secret_key"</code></pre>

                <h3>Basic Usage</h3>
                <pre><code class="language-python">from qianfan import QianfanVL
import base64

# Initialize client
client = QianfanVL(
    access_key="YOUR_ACCESS_KEY",
    secret_key="YOUR_SECRET_KEY"
)

# Load and encode image
with open("image.jpg", "rb") as f:
    image_base64 = base64.b64encode(f.read()).decode()

# Generate response
response = client.chat.completions.create(
    model="Qianfan-VL-8B",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "Describe this image in detail."},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
        ]
    }],
    max_tokens=500,
    temperature=0.7
)

print(response.choices[0].message.content)</code></pre>

                <h3>Advanced Features</h3>
                
                <h4>Multi-turn Conversation</h4>
                <pre><code class="language-python"># Create a session for multi-turn dialogue
session = client.create_session()

# First turn
response1 = session.chat(
    image="chart.png",
    message="What does this chart show?"
)

# Second turn (maintains context)
response2 = session.chat(
    message="What's the trend between Q1 and Q2?"
)</code></pre>

                <h4>Batch Processing</h4>
                <pre><code class="language-python">import asyncio
from qianfan import AsyncQianfanVL

async def batch_process(images, prompts):
    client = AsyncQianfanVL()
    tasks = [
        client.analyze(img, prompt)
        for img, prompt in zip(images, prompts)
    ]
    return await asyncio.gather(*tasks)

# Process multiple images concurrently
results = asyncio.run(batch_process(images, prompts))</code></pre>
            </section>

            <section id="api" class="section">
                <h2>API Reference</h2>
                
                <h3>REST API Endpoint</h3>
                <pre><code class="language-bash">POST https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/qianfan_vl_8b</code></pre>

                <h3>Request Format</h3>
                <pre><code class="language-json">{
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "Your question here"},
        {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
      ]
    }
  ],
  "temperature": 0.7,
  "max_tokens": 500,
  "top_p": 0.95
}</code></pre>

                <h3>Response Format</h3>
                <pre><code class="language-json">{
  "id": "chat-abc123",
  "object": "chat.completion",
  "created": 1693435600,
  "model": "Qianfan-VL-8B",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The generated response text..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 256,
    "completion_tokens": 128,
    "total_tokens": 384
  }
}</code></pre>

                <h3>Parameters</h3>
                <table class="api-table">
                    <thead>
                        <tr>
                            <th>Parameter</th>
                            <th>Type</th>
                            <th>Default</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><code>model</code></td>
                            <td>string</td>
                            <td>required</td>
                            <td>Model identifier (e.g., "Qianfan-VL-8B")</td>
                        </tr>
                        <tr>
                            <td><code>messages</code></td>
                            <td>array</td>
                            <td>required</td>
                            <td>Array of message objects</td>
                        </tr>
                        <tr>
                            <td><code>temperature</code></td>
                            <td>float</td>
                            <td>0.7</td>
                            <td>Sampling temperature (0.0 to 2.0)</td>
                        </tr>
                        <tr>
                            <td><code>max_tokens</code></td>
                            <td>integer</td>
                            <td>2048</td>
                            <td>Maximum tokens to generate</td>
                        </tr>
                        <tr>
                            <td><code>top_p</code></td>
                            <td>float</td>
                            <td>0.95</td>
                            <td>Nucleus sampling parameter</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section id="use-cases" class="section">
                <h2>Use Cases</h2>
                
                <div class="use-cases-grid">
                    <div class="use-case">
                        <h3>Document Intelligence</h3>
                        <p>Extract structured information from PDFs, invoices, forms, and handwritten documents with high accuracy.</p>
                        <code>model.analyze_document(pdf_path, extract_tables=True)</code>
                    </div>
                    
                    <div class="use-case">
                        <h3>Visual Question Answering</h3>
                        <p>Answer complex questions about images, charts, and diagrams with detailed explanations.</p>
                        <code>model.vqa(image, "What is the main topic?")</code>
                    </div>
                    
                    <div class="use-case">
                        <h3>E-commerce Applications</h3>
                        <p>Generate product descriptions, analyze user photos, and provide visual search capabilities.</p>
                        <code>model.generate_product_desc(product_image)</code>
                    </div>
                    
                    <div class="use-case">
                        <h3>Educational Tools</h3>
                        <p>Help students understand complex diagrams, solve visual problems, and provide tutoring.</p>
                        <code>model.explain_diagram(educational_image)</code>
                    </div>
                </div>
            </section>

            <section id="roadmap" class="section">
                <h2>Development Roadmap</h2>
                
                <div class="timeline">
                    <div class="timeline-item">
                        <div class="timeline-date">Q1 2025</div>
                        <div class="timeline-content">
                            <strong>Qianfan-VL-70B Release</strong>
                            <p>Larger model with enhanced reasoning capabilities</p>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-date">Q2 2025</div>
                        <div class="timeline-content">
                            <strong>Video Understanding</strong>
                            <p>Support for video input and temporal reasoning</p>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-date">Q3 2025</div>
                        <div class="timeline-content">
                            <strong>Open Source Release</strong>
                            <p>Selected model weights available to researchers</p>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-date">Q4 2025</div>
                        <div class="timeline-content">
                            <strong>Industry Models</strong>
                            <p>Specialized versions for healthcare, finance, and retail</p>
                        </div>
                    </div>
                </div>
            </section>

            <section id="resources" class="section">
                <h2>Resources & Support</h2>
                
                <div class="resources-grid">
                    <div class="resource-card">
                        <h3>Documentation</h3>
                        <ul>
                            <li><a href="https://cloud.baidu.com/doc/WENXINWORKSHOP">API Documentation</a></li>
                            <li><a href="https://github.com/baidubce/qianfan-models-cookbook">Code Examples</a></li>
                            <li><a href="https://console.bce.baidu.com/qianfan">Console</a></li>
                        </ul>
                    </div>
                    
                    <div class="resource-card">
                        <h3>Community</h3>
                        <ul>
                            <li><a href="https://ai.baidu.com/forum">Discussion Forum</a></li>
                            <li><a href="https://github.com/baidubce">GitHub Organization</a></li>
                            <li>WeChat: 百度智能云千帆</li>
                        </ul>
                    </div>
                    
                    <div class="resource-card">
                        <h3>Contact</h3>
                        <ul>
                            <li>Technical: qianfan-support@baidu.com</li>
                            <li>Business: qianfan-biz@baidu.com</li>
                            <li>Research: qianfan-research@baidu.com</li>
                        </ul>
                    </div>
                </div>

                <div class="cta-section">
                    <h3>Get Started with Qianfan-VL</h3>
                    <p>Experience the power of advanced vision-language understanding</p>
                    <a href="https://console.bce.baidu.com/qianfan" class="btn btn-primary">Access Console</a>
                    <a href="https://cloud.baidu.com/doc/WENXINWORKSHOP" class="btn btn-secondary">Read Documentation</a>
                </div>
            </section>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-info">
                    <p>&copy; 2025 Baidu, Inc. All rights reserved.</p>
                    <p>Qianfan-VL is a product of Baidu Cloud.</p>
                </div>
                <div class="footer-links">
                    <a href="https://cloud.baidu.com">Baidu Cloud</a>
                    <a href="https://ai.baidu.com">Baidu AI</a>
                    <a href="https://ir.baidu.com">Investor Relations</a>
                    <a href="https://www.baidu.com/duty">Legal</a>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
</body>
</html>